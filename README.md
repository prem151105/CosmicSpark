# CosmicSpark: KnowledgeForge AI

CosmicSpark is an advanced knowledge management and retrieval system that combines state-of-the-art NLP, graph-based knowledge representation, and interactive visualization to create a powerful platform for information extraction, processing, and exploration.

## ğŸš€ Key Innovations

1. **Hybrid Knowledge Representation**
   - Combines vector embeddings with graph-based relationships
   - Dynamic schema adaptation based on document content
   - Multi-hop reasoning across connected entities

2. **Advanced NLP Pipeline**
   - Custom entity recognition with domain adaptation
   - Contextual relationship mapping
   - Multi-modal processing (text, PDFs, web content)

3. **Intelligent Search & Retrieval**
   - Hybrid search combining BM25 with dense vector search
   - Context-aware result reranking
   - Query understanding and expansion

## ğŸ› ï¸ Prerequisites

- Python 3.9+
- pip (Python package manager)
- (Optional) Neo4j Database for graph features
- (Recommended) CUDA-enabled GPU for faster inference

## ğŸš€ Quick Start

1. **Clone and Setup**
   ```bash
   git clone https://github.com/prem151105/CosmicSpark.git
   cd CosmicSpark
   
   # Create and activate virtual environment
   python -m venv venv
   .\venv\Scripts\activate  # Windows
   source venv/bin/activate  # Linux/Mac
   
   # Install dependencies
   pip install -r requirements_clean.txt
   ```

2. **Configure Environment**
   Copy `.env.example` to `.env` and update the settings:
   ```bash
   cp .env.example .env
   ```
   
   Edit `.env` to configure:
   - API settings
   - Database connections
   - Model parameters

## ğŸš€ Running the Application

### Backend (FastAPI)
```bash
# Start the API server
python -m src.api.main
```

### Frontend (Streamlit)
```bash
# In a new terminal
streamlit run frontend/streamlit_app.py
```

Access the application at `http://localhost:8501`

## ğŸ—ï¸ Project Structure

```
CosmicSpark/
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ chromadb/           # Vector embeddings
â”‚   â”œâ”€â”€ knowledge_graph/    # Graph database files
â”‚   â”œâ”€â”€ processed/          # Processed documents
â”‚   â””â”€â”€ raw/                # Raw input data
â”‚
â”œâ”€â”€ frontend/               # Streamlit UI
â”‚   â”œâ”€â”€ components/         # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ chat_interface.py
â”‚   â”‚   â””â”€â”€ knowledge_graph_viz.py
â”‚   â””â”€â”€ streamlit_app.py    # Main application
â”‚
â”œâ”€â”€ src/                    # Core application code
â”‚   â”œâ”€â”€ api/                # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ endpoints.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚
â”‚   â”œâ”€â”€ crawler/            # Web crawling and processing
â”‚   â”‚   â”œâ”€â”€ content_extractor.py
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â”‚   â””â”€â”€ web_crawler.py
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge_graph/    # Knowledge graph components
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py
â”‚   â”‚   â”œâ”€â”€ kg_builder.py
â”‚   â”‚   â””â”€â”€ relationship_mapper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/             # ML models
â”‚   â”‚   â”œâ”€â”€ embedding_model.py
â”‚   â”‚   â””â”€â”€ llm_handler.py
â”‚   â”‚
â”‚   â””â”€â”€ rag/                # Retrieval-Augmented Generation
â”‚       â”œâ”€â”€ generator.py
â”‚       â”œâ”€â”€ retriever.py
â”‚       â””â”€â”€ vector_store.py
â”‚
â”œâ”€â”€ .env.example           # Example environment config
â”œâ”€â”€ requirements_clean.txt # Dependencies
â””â”€â”€ README.md             # This file
```

## ğŸ› ï¸ Configuration

### Environment Variables

```env
# Server Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# Vector Store
VECTOR_STORE_PATH=./data/chromadb
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Knowledge Graph (Optional)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# LLM Configuration
LLM_MODEL=llama3
LLM_TEMPERATURE=0.7
MAX_TOKENS=1024
```

## ğŸš€ Deployment

### Production Setup
1. Set up a production-grade server (Nginx/Apache)
2. Use Gunicorn/Uvicorn for ASGI server
3. Configure environment variables in production
4. Set up monitoring and logging



## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [CosmicSpark](LICENSE) file for details.

## âœ¨ Key Features in Action

- **Real-time Knowledge Graph Updates**: See relationships form as you add documents
- **Context-Aware Chat**: Get precise answers with source attribution
- **Document Intelligence**: Extract and link entities across multiple documents
- **Scalable Architecture**: Built to handle thousands of documents

## ğŸ“š Documentation

For detailed documentation, please refer to the [docs](docs/) directory.

## ğŸ“¬ Contact

For questions or feedback, please open an issue or contact the maintainers.
